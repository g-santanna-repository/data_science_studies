{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Complexidade de Modelos - Viés e Variância\n \n## Observações abaixo - função polinomial de 1 grau com ruido","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nimport seaborn as sns\nfrom scipy import interpolate\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\n\n\nx_reta = np.arange(1, 5, 0.1)\nruido = (np.random.rand(len(x_reta))) / 1.5\ny_reta = x_reta + ruido\n\nsns.scatterplot( x = x_reta, y = y_reta);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"polinomial_fitted_reta = np.polynomial.polynomial.Polynomial.fit(x_reta, y_reta, 1)\n\nplt.plot(x_reta, y_reta, 'o', x_reta, polinomial_fitted_reta(x_reta), '-')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"O modelo acima é o que melhor descreve o comportamento dos dados, pois consegue captar a função que gera as observação, ao qual é somada um pouco de ruido.\n\nObservação:\n\n1) Mesmo obtendo a função de fundo que gerou os dados, existe um ruido aleatorio junto ao valor de cada observação.\n        \n2) Não é possivel prever o ruido aleatório que corresponde com cada nova observação gerada.","metadata":{"execution":{"iopub.status.busy":"2022-11-17T12:59:56.646069Z","iopub.execute_input":"2022-11-17T12:59:56.646698Z","iopub.status.idle":"2022-11-17T12:59:56.683267Z","shell.execute_reply.started":"2022-11-17T12:59:56.646572Z","shell.execute_reply":"2022-11-17T12:59:56.681743Z"}}},{"cell_type":"code","source":"f = interpolate.interp1d(x_reta, y_reta)\n\nxnew = np.arange(x_reta.min(), x_reta.max(), 0.1)\nynew = f(xnew)   # use interpolation function returned by `interp1d`\nplt.plot(x_reta, y_reta, 'o', xnew, ynew, '-')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"O modelo acima tem Erro = 0 nos dados de treino, pois passa exatamente em cima de cada um dos pontos - porem é bastante intuitivo perceber que esse modelo é complexo demais e não deve apresentar uma boa generalização para prever valores de novos dados.\n\nO modelo se adapta completamente ao ruido presente nas observações, ficando excessivamente complexo e diminuindo sua capacidade de generalização para novas observações.\n\nEsse modelo coloca a noção de 2 questões importantes:\n\n- A complexificação dos modelos permite zerar o erro nos dados de treino, porem isso não necessariamente representa capacidade de generalização dessa medida de erro.\n\n- A necessidade de utilizar outro conjunto de dados, separado dos dados de treinamento, para estimar mais fielmente o erro de generalização do modelo.\n\n## Dados abaixo - função polinomial de 3 grau com ruido","metadata":{"execution":{"iopub.status.busy":"2022-11-17T13:00:42.035358Z","iopub.execute_input":"2022-11-17T13:00:42.035921Z","iopub.status.idle":"2022-11-17T13:00:42.046585Z","shell.execute_reply.started":"2022-11-17T13:00:42.035871Z","shell.execute_reply":"2022-11-17T13:00:42.044403Z"}}},{"cell_type":"code","source":"x = np.arange(1, 5, 0.1)\nruido = (np.random.rand(len(x)) * 25)\ny = x * x * x + ruido\n\nsns.scatterplot( x = x, y = y);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ao utilizar uma regressão linear\n\n- Os dados são gerados por uma função mais complexa do que a estimada pelo modelo, fazendo com que a estimativa esteja na média bastante errada. Dizemos que esse modelo tem alto viés dado que as estimativas de valores são, em média, relativamente distantes dos valores reais.","metadata":{}},{"cell_type":"code","source":"polinomial_fitted = np.polynomial.polynomial.Polynomial.fit(x, y, 1)\n\nplt.plot(x, y, 'o', x, polinomial_fitted(x), '-')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Tanto esse modelo abaixo (de uma função cubica para dados gerados por uma função cubica) quanto o primeiro modelo apresentado (uma regressão linear para dados gerados linearmente) apresentam modelos que consideramos ter baixo viés, dado que as estimativas dos modelos estão bastante próximas dos valores reais observados.","metadata":{}},{"cell_type":"code","source":"polinomial_fitted = np.polynomial.polynomial.Polynomial.fit(x, y, 3)\n\nplt.plot(x, y, 'o', x, polinomial_fitted(x), '-')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - Já esse modelo abaixo, assim como o modelo da primeira situação com Erro = 0, são gerados por polinomios de grau muito alto, que são muito instaveis nas suas estimativas, mesmo quando ocorrem pequenissimas alterações nos valores de entrada - modelando mesmo o erro aleatório de cada observação nas suas estimativas. \n\n- Ao serem tão instaveis e variarem enormemente mesmo com pequenas alterações nos dados de treino, dizemos nesse problema que são modelos com alta variância.","metadata":{}},{"cell_type":"code","source":"polinomial_fitted = np.polynomial.polynomial.Polynomial.fit(x, y, len(x))\n\nplt.plot(x, y, 'o', x, polinomial_fitted(x), '-')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Apresentação mais intuitiva de Viés e Variância\n\n### Todo erro de modelos supervisionados pode ser decomposto em três partes:\n\n                                Erro do Modelo = Viés + Variância + Erro Irredutivel\n                                \nIntuitivamente, o Erro Irredutivel é aquele sobre o que não é possivel intervir e constitui o erro minimo que qualquer modelo consegue obter nessa tarefa.\n\nA parte do erro por Viés esta associada ao erro do modelo em conseguir captar relações entre as variaveis de entrada e a target - como por utilizar relações simplificadas demais (como relações lineares em cenários não-lineares) ou devido a baixo poder explicativo das variaveis de entrada para estimar a target.\n\nA parte do erro por Variância esta associado ao erro do modelo cometido pela não capacidade de generalização nas relações aprendidas - como por modelos que se adequam perfeitamente ao ruido ou em particularidades presente nos dados de treino.","metadata":{}},{"cell_type":"markdown","source":"### Conceitos melhor formalizados:\n\nSendo:\n\n- x0 uma observação qualquer\n\n- f a função que nosso modelo retorna ao estimar uma observação\n\n- w a função original que gerou os dados observados \n\n- E[x] o valor médio de x                                        \n\n                                            Viés = E[f(x0)] − w(x0)\n\n\nOu seja, o viés corresponde ao valor em média que nossa predição difere do valor verdadeiro de target.                                        \n\n                                     Variância = E[ f(x0) − E[f(x0)] ]²\n                                    \n                                    \nOu seja, a variância corresponde ao desvio quadrático médio de quanto as predições do modelo diferem de sua predição média.\n\n\nO Erro Irredutivel: é o menor erro possivel de ser alcançado para qualquer modelo, dado a aleatoriedade não explicavel que compoe os resultados das observações. Tambem é chamado de Erro Bayesiano (ou Bayes Error Rate)\n\n- Observação: Enquanto o viés é calculado levando em conta o valor real da observação e o valor estimado, a variãncia leva apenas em consideração os valores estimados gerados pelo próprio modelo\n\n#### Perspectiva mais estatística:\n\n- Viés e Variância são o primeiro e segundo momento (média e variância) obtidos de uma distribuição de probabilidade, gerada pelos resultados de seu modelo.\n\n- Assim como para distribuições de probabilidade em geral, o primeiro e segundo momento podem estar relacionados (como no caso de uma Distribuição Normal) mas não necessariamente sempre estão - podendo gerar diferentes situações como, alto viés e alta variancia, alto viés e baixa variância, baixo viés e baixa variância, baixo viés e alta variância - porem, geralmente nos modelos de classificações/regressões existe uma relação em que ações voltadas para a diminuição do viés aumentam a variância e vice-versa.\n\n- Destaque no geralmente, pois é bastante simples construir exemplo em que alterações reduzam tanto o viés quanto a variância original do modelo, ao mesmo tempo.","metadata":{}},{"cell_type":"markdown","source":"# Overfitting e Underfitting\n    \n## Overfitting - Problema de variância\n\n- Overfitting se refere a um modelo que se adaptou excessivamente ao conjunto de treinamento, não tendo capacidade de generalização da performance.\n\n- Acontece quando um modelo aprende as particularidades e ruidos dos dados de treinamento, que não se extendem para novos dados e assim tem sua performance impactada.\n\n- Geralmente ocorre quando existem relativamente poucas observações, quando um numero muito grande de variaveis é utilizado em alguns modelos e/ou com o uso de modelos excessivamente complexos para o problema.\n\n## Underfitting - Problema de viés\n\n- Underfitting se refere a um modelo que não consegue se adaptar bem nem ao conjunto de treinamento, tendo alto erro mesmo nesses dados.\n\n- Acontece quando o modelo não teve a capacidade construir boas relações entre as variaveis e a target, sendo um modelo excessivamente simples para representar a situação.\n\n- Geralmente ocorre quando utilizamos modelos com pressupostos que não são validos no problema modelado ou quando temos variaveis muito pouco relacionadas com a target.","metadata":{}},{"cell_type":"markdown","source":"# Regularização\n\n\nSão técnicas voltadas para penalizar a construção de modelos complexos, de maneira a obter modelos mais simples  e reduzir overfitting.\n\nCada tipo de modelo costuma ter métodos especificos para sua regularização, que adicionam restrições para a construção do modelo, de forma a obter modelos com maior capacidade de generalização.\n\nSegue exemplo abaixo:\n\n- Utilizando o conjunto de dados gerado pela função de terceiro grau e é possivel observar o comportamento de uma arvore de decisão ao variar seu principal hyperparametro: A profundidade da arvore (que corresponde a quantos \"cortes\" ela realiza)\n\n- Se deixarmos profundidade = 0, o modelo retonará o valor médio de todos os dados, como estimativa para qualquer novo dado apresentado\n\n- Se deixarmos profundidades muito grandes, a tendencia do modelo é realizar uma quebra para cada uma das observações nos dados apresentados, se adaptando muito fortemente ao ruido e tendo pouca estabilidade.\n\n- Os parametros reg1 (min_samples_leaf) e reg2 (ccp_alpha) são hyperparametros de regularização do modelo, que penalizam a construção de modelos complexos e simplificam os cortes da arvore. \n\n- O primeiro corresponde com o numero minimo de observações que é necessário para que possa ser possivel realizar um corte da arvore de decisão, o segundo corresponde a um hyperparametro que mede a complexidade da arvore e de junções possiveis entre os cortes realizados, retornando arvores mais simples ao agregar os cortes finais.\n\n- É interessante observar que se utilizarmos uma regularização muito forte nesse problema, nossa arvore de decisão retornará novamente o mesmo resultado obtido quando profundidade = 0; porque a penalização se torna excessivamente alta para construção de qualquer modelo mais complexo.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nx = np.array(np.arange(1, 5, 0.05))\nruido = np.random.rand(len(x)) * 25\n\ny = x * x * x + ruido\n\nplt.plot(x, y, 'o')\n\nprofundidade = 10\nreg1 = 1 # valor inicial é 1 - mean samples leaf\nreg2 = 0 # valor inicial é 0 - pruning\n\ndtr = DecisionTreeRegressor(max_depth = profundidade, min_samples_leaf = reg1, ccp_alpha = reg2)\ndtr.fit(x.reshape(-1,1),y.reshape(-1,1))\n\n\nplt.plot(x, dtr.predict(x.reshape(-1,1)));","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Abordagens para redução de viés:\n \n1) Utilizar um modelo mais complexo.\n\n2) Complexificar o modelo atual, a partir de seus hyperparametros ou relaxamento de pressupostos.\n\n3) Reduzir ou eliminar a regularização.\n\n4) Melhor tratamento das features utilizadas.\n\n5) Adição ou construção de novas features com capacidade preditiva.\n\n6) Uso de ensembles voltados a redução de viés (como boosting ou stacking).","metadata":{}},{"cell_type":"markdown","source":"## Abordagens para redução de variância:\n\n1) Utilizar um modelo mais simples.\n\n2) Regularizar o modelo utilizado.\n\n3) Diminuir a quantidade de features utilizadas.\n\n4) Tratar o ruido das variaveis de entrada.\n\n5) Uso de ensembles voltados para redução de variância (como bagging).","metadata":{}},{"cell_type":"markdown","source":"## Exemplos interessantes na relação viés/variância:\n \nPodem tanto aumentar viés e variância, quanto reduzir os dois ao mesmo tempo.\n\n1) Construção de novas features.\n\n2) Segmentação do problema.","metadata":{}},{"cell_type":"markdown","source":"#  Exemplo: Case Data Masters\n\n##  Escolha a partir das caracteristicas do projeto\n    \n- Variaveis estarem anonimizadas restringem interpretabilidade.\n\n- Pelo pouco tempo disponivel, preferencia por modelos de alta performance com menor necessidade de tratamento ou construção de variáveis.\n\n- Modelos com treino/teste rápido para revalidações e sua otimização dentro do prazo\n\n- Modelos que sejam escalaveis para maior quantidade de dados, imaginando que esse seria um modelo para aplicar em uma base grande de clientes\n\n## Escolhido : Random Forest\n\n- Complexidade computacional baixa para treino (ou seja, roda rapido para treinar em grandes conjuntos de dados) e opção de paralização para treino/otimização\n\n- Não é sensivel a outliers\n\n- É invariante em relação a escala das variáveis\n\n- Detecta interações entra variaveis e lida bem com relações não lineares\n\n- Alta performance\n\n- Rapidez de scoragem dos dados\n\n- Escalavel para grande quantidade de dados","metadata":{}},{"cell_type":"markdown","source":"# Técnicas de Diagnóstico\n\n 1) Validation Curves permitem analisar a influencia de um unico hyperparametro - geralmente o principal hyperparametro do algoritmo e mais significativo - no trade-off de viés/variancia, nos fornecendo uma noção baseline sobre sua performance.    \n \n     - Exemplos de hyperparametros mais usados são Profundidade Máxima em Arvores de decisão, Numero de vizinhos em KNN, peso da Regularização Lasso ou Ridge em Modelos Lineares, peso do Hyperparametro C para regularização em SVMs, etc.\n\n2) Learning Curves permitem analisar, ao fixar todos os hyperaparametros de um modelo, se suas estimativas sofrem de maior erro de viés e/ou variância.    \n\n    - Nesse caso, todos os hyperparametros do modelo permanecem fixados e a variação progressiva ocorre na quantidade de observações utilizadas para treino e para teste.\n\n3) Essas técnicas são utilizadas para considerar se o modelo escolhido aparenta ser um bom ponto de partida para resolução do problema, assim como dar orientação sobre os caminhos para melhoras do modelo.4) Validation Curves são, em geral, realizadas antes de Learning Curves para que a informação de performance fornecida sobre o hyperparametro principal seja utilizado nas Learning Curves. ","metadata":{}},{"cell_type":"markdown","source":"### Validation Curve (Random Forest) - Max Depth\n","metadata":{}},{"cell_type":"code","source":"%%time\n\ntrain_scores, test_scores = validation_curve(RandomForestClassifier(), \n                                             X_train_clean, \n                                             y_train, \n                                             param_name = \"max_depth\", \n                                             param_range = param_range,\n                                             cv = 7, \n                                             scoring = \"roc_auc\", \n                                             n_jobs = -1)\n\n\n# Calculate mean and standard deviation for training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Calculate mean and standard deviation for test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Plot mean accuracy scores for training and test sets\nplt.plot(param_range, train_mean, label=\"Training score\", color=\"black\")\nplt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"dimgrey\")\n\n# Plot accurancy bands for training and test sets\nplt.fill_between(param_range, train_mean - train_std, train_mean + train_std, color=\"gray\")\nplt.fill_between(param_range, test_mean - test_std, test_mean + test_std, color=\"gainsboro\")\n\n# Create plot\nplt.title(\"Validation Curve With RF\")\nplt.xlabel(\"Max Depth\")\nplt.ylabel(\"ROC_AUC Score\")\nplt.tight_layout()\nplt.legend(loc=\"best\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" 1) Na Validation Curve da Random Forest acima é possivel observar o comportamento das curvas de performance no treino e no teste, conforme aumentamos a complexidade do modelo.\n \n 2) No lado esquerdo do gráfico, é onde se encontra a Random Forest mais simples entre os valores buscados e no lado direito, a Random Forest mais complexa.\n \n- Podemos observar que quando o modelo se encontra mais simples possivel no lado esquerdo, a performance no treino é a menor entre todos os valores e se encontra colada na performance do teste. Nesse ponto, o modelo tem baixa variancia e alto viés.\n\n- Conforme complexificamos o modelo, a performance do treino cresce progressivamente, junto da performance do teste - o que significa que o viés do modelo vai diminuindo - porem, tambem cresce a separação na difereça de performance entre o treino e a do teste - indicando tambem um aumento de variância do modelo.\n\n- A partir de um certo ponto, ao redor da profundidade 8, a performance do teste para de crescer - o que indica que não ocorre mais  diminuição do viés - porem a separação entre a performance no treino e no teste segue crescendo ao complexificarmos o modelo - indicando que a variancia continua crescendo.\n\n- O ponto ótimo a ser escolhido é na região em que a performance no teste é a maior possivel - ou seja, quando o modelo alcança o ponto de menor viés - e a separação entre treino e teste é a menor - ou seja, tambem com baixa variância.\n\n3) Esse ponto ótimo do gráfico é geralmente um bom baseline de referencia sobre a performance do modelo, mas não necessariamente o resultado final a ser utilizado, porque:\n\n- Os resultados de performance obtidas no grafico são uma estimativa das performances reais, em que a zona cinza indica uma aproximação de intervalo de confiança; portanto existe incerteza associada a esse ponto ótimo.\n\n- Existem outros hyperparametros dos modelos que podem ser variados, assim como técnicas que buscam reduzir a variância e o viés dos modelos .","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(RandomForestClassifier(max_depth = 9), \n                                                        X_train_clean, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv = 10,\n                                                        # Evaluation metric\n                                                        scoring = 'roc_auc',\n                                                        # Use all computer cores\n                                                        n_jobs = -1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes = np.linspace(0.05, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"ROC-AUC Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Learning Curves\n\n1) São produzidas ao fixar um modelo e variar a quantidade de dados utilizados para treino.\n\n2) Distancia entre performance de treino e o teste reflete o patamar de variância do modelo.\n\n    - Modelos com muita variancia apresentam curvas bastante separadas, enquanto modelos com baixa variancia apresentam curvas muito próximas.\n\n3) O ponto de convergência para onde a curva de performance entre treino e teste se direcionam reflete o nivel de viés do modelo.\n\n    - Modelos com baixo viés convergem para um ponto de performance alto entre treino/teste, enquanto modelos com muito viés convergem para uma baixa performance.\n    \n\n- Dado a performance observa entre treino e teste nos modelos abaixo, o diagnostico inicial indica que a RF parece ser uma boa escolha de modelo baseline do qual seguir.\n\n\n- O ponto de convergencia de performance da RF é ao redor de 85-86% de AUC, o que é uma métrica de partida bastante alta para um modelo de classificação e invariante em relação ao desbalanceamento do dataset.\n\n- Dado que o ponto de convergencia parece satisfatório, o curso de ação mais adequado é pela redução da variância, de forma a fazer as duas curvas convergirem como prioridade.\n\n- A validation curve indica um range ao redor de 6 a 11 como uma boa referencia de busca por melhor profundidade\n\n\nAs técnicas priorizadas serão as seguintes:\n\n    - Regularização\n    - Seleção de melhor combinação de features, diminuindo o numero utilizado agora\n    - Oversampling\n         \n- Caso esse AUC for julgado ainda inadequado, outras abordagens seriam priorizadas para buscar aumentar o ponto de convergencia entre performance treino/teste\n\n    Como: \n    \n     - Feature Extraction\n     - Construção e transformação de features\n     - Analise de interações entre features e construções de variaveis voltadas para isso\n     - Uso de ensembles\n     - Relaxamento parcial nos filtros anteriores aplicados nas features, permitindo mais features serem escolhidas\n     - Segmentação do problema\n     - Uso de um modelo mais complexo/eficiente (talvez recorrendo a boosting)             \n","metadata":{}},{"cell_type":"code","source":"param_range = np.linspace(0.05, 30, 20)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nfrom sklearn.linear_model import LogisticRegression\n\ntrain_scores, test_scores = validation_curve(LogisticRegression(max_iter = 200), \n                                             X_train_clean, \n                                             y_train, \n                                             param_name = \"C\", \n                                             param_range = param_range,\n                                             cv = 5, \n                                             scoring = \"roc_auc\",  \n                                             n_jobs = -1)\n\n\n# Calculate mean and standard deviation for training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Calculate mean and standard deviation for test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Plot mean accuracy scores for training and test sets\nplt.plot(param_range, train_mean, label=\"Training score\", color=\"black\")\nplt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"dimgrey\")\n\n# Plot accurancy bands for training and test sets\nplt.fill_between(param_range, train_mean - train_std, train_mean + train_std, color=\"gray\")\nplt.fill_between(param_range, test_mean - test_std, test_mean + test_std, color=\"gainsboro\")\n\n# Create plot\nplt.title(\"Validation Curve With LG\")\nplt.xlabel(\"Regularization Parameter\")\nplt.ylabel(\"ROC_AUC Score\")\nplt.tight_layout()\nplt.legend(loc=\"best\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1) Tanto a performance do treino, quanto do teste, são bastante baixas, o que indica que o modelo é bastante enviesado (ou seja, a difereça entre os valores previstos pelo modelo e os valores reais das observaçõs é alta, em média).\n\n2) A performance do treino sempre esta muito colada com a performance do teste, o que indica que o modelo tem baixissima variancia.\n\n3) Existem diversos motivos possiveis sobre o porque esse comportamento se manteve por todo o gráfico, como:\n\nPressupostos do Modelo (Regressão Logistica) \n\n    a) Linearidade na relação com log-odds da target.  \n    \n    b) Desconsiderar interações entre variaveis e considerar contribuição independente de cada variavel.  \n    \n    c) Observações não serem geradas de forma independente.  \n    \n    d) Observações não serem identicamente distribuidas.\n    \n    - Variaveis estão em escalas diferentes e regularização L1/L2 não é invariante em relação a escala das variaveis.\n    \n    - Existencia de outliers muito extremos.","metadata":{}},{"cell_type":"code","source":"param_range = np.arange(1, 51, 2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ntrain_scores, test_scores = validation_curve(KNeighborsClassifier(), \n                                             X_train_clean, \n                                             y_train, \n                                             param_name = \"n_neighbors\", \n                                             param_range = param_range,\n                                             cv = 7, \n                                             scoring = \"roc_auc\",  \n                                             n_jobs = -1)\n\n\n# Calculate mean and standard deviation for training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Calculate mean and standard deviation for test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Plot mean accuracy scores for training and test sets\nplt.plot(param_range, train_mean, label=\"Training score\", color=\"black\")\nplt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"dimgrey\")\n\n# Plot accurancy bands for training and test sets\nplt.fill_between(param_range, train_mean - train_std, train_mean + train_std, color=\"gray\")\nplt.fill_between(param_range, test_mean - test_std, test_mean + test_std, color=\"gainsboro\")\n\n# Create plot\nplt.title(\"Validation Curve With KNN\")\nplt.xlabel(\"K Neighbours\")\nplt.ylabel(\"ROC_AUC Score\")\nplt.tight_layout()\nplt.legend(loc=\"best\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1) A posição do \"modelo simples\" e \"modelo complexo\" é o inverso da posição observada no gráfico da Random Forest - nesse gráfico, o modelo \"mais complexo\" se encontra na esquerda e o modelo \"mais simples\" na direita.\n\n2) Conforme a distancia entre a performance do treino e teste diminui, tanto a performance de treino quanto do teste ficam bastante abaixo das performances observadas no grafico da Random Forest - indicando que existe um erro consideravel de viés nesse modelo. \n\n3) Durante todos os valores utilizado para o K, se mantem uma distancia grande entre a performance de treino e a de teste - indicando tambem erro consideravel de variância.\n\nMotivos diversos para os erros de viés e os de variância:  \n        \n        a) Escala das features.        \n        b) Features que não contribuem para separabilidade tem mais peso em um KNN do que em Arvores de Decisão.          \n        c) Geometria utilizada.   \n        d) Outliers e amostras não representativas na zona de separação entre as classes.\n        e) Uso de peso uniforme, em vez de ponderação pela distancia, como hyperparametro.\n        f)  Observações não serem geradas de forma independente.          \n        g) Observações não serem identicamente distribuidas.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}